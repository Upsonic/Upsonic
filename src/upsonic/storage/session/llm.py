from __future__ import annotations
import time
from typing import Any, Dict, List, Literal, Optional
from uuid import uuid4

from pydantic import BaseModel, Field



class Artifact(BaseModel):
    """
    Represents the metadata for a non-textual artifact (e.g., an image,
    document, or audio file) associated with an LLMConversation.

    The binary data itself is stored in a separate blob store (like S3 or a
    local file system), and this model holds the pointer and metadata.
    """
    # A unique identifier for the artifact.
    artifact_id: str = Field(default_factory=lambda: str(uuid4()))

    # Foreign key linking this artifact to a specific conversation.
    conversation_id: str

    # (Optional) Foreign key linking this artifact to a specific turn
    # within the conversation (e.g., an image generated by the assistant).
    turn_id: Optional[str] = None

    # The MIME type of the artifact, e.g., "image/png", "application/pdf".
    mime_type: str

    # The URI pointing to the artifact's actual location in a blob store.
    storage_uri: str

    # A flexible key-value store for artifact-specific metadata.
    metadata: Dict[str, Any] = Field(default_factory=dict)



class LLMUsageStats(BaseModel):
    """
    A structured model for capturing token usage statistics from an LLM call.
    """
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0


class LLMToolCall(BaseModel):
    """
    A structured model representing a single tool/function call requested
    by an LLM, and the subsequent result.
    """
    # The name of the tool or function that the LLM requested to call.
    tool_name: str

    # The arguments for the tool call, as a JSON-serializable dictionary.
    # The LLM typically provides this as a JSON object.
    arguments: Dict[str, Any]

    # The result or output from executing the tool. This is what gets sent
    # back to the LLM in the next turn. Can be any JSON-serializable type.
    tool_output: Optional[Any] = None


class LLMTurn(BaseModel):
    """
    The atomic unit of storage for an LLM interaction.

    This model captures a single, complete request-response cycle, ensuring
    every detail of the interaction is recorded for logging, debugging,
    and potential fine-tuning.
    """
    # A unique identifier for this specific turn within a conversation.
    turn_id: str = Field(default_factory=lambda: str(uuid4()))

    # High-precision timestamps for latency and ordering analysis.
    timestamp_start: float = Field(default_factory=time.time)
    timestamp_end: Optional[float] = None

    # The role of the entity responsible for this turn's content.
    role: Literal["user", "assistant", "system", "tool"]

    # The primary textual content of the turn (e.g., user's query, LLM's text response).
    content: Optional[str] = None


    # The raw, unprocessed prompt template (e.g., "Summarize this: {text}").
    prompt_template: Optional[str] = None

    # The dictionary of variables that were rendered into the prompt template.
    prompt_template_vars: Optional[Dict[str, Any]] = None

    # The final, fully-rendered prompt that was sent to the LLM API.
    # Storing this is critical for exact reproducibility.
    final_prompt: Optional[str] = None


    # The raw, complete JSON object returned from the LLM provider's API.
    # This is a "save everything" field to prevent data loss.
    llm_response: Optional[Any] = None

    # The exact configuration used for the LLM call, ensuring perfect reproducibility.
    llm_config: Optional[Dict[str, Any]] = None # e.g., {"model": "gpt-4-turbo", "temperature": 0.5}

    # Structured model for token usage statistics.
    usage_stats: Optional[LLMUsageStats] = None

    # The calculated cost of this turn in a reference currency (e.g., USD).
    cost: Optional[float] = None

    # A list of structured tool calls made and their results within this turn.
    tool_calls: Optional[List[LLMToolCall]] = None

    # Any error message or traceback if the turn failed at any point.
    error: Optional[str] = None

    # A flexible key-value store for any other custom data, tags, or IDs.
    metadata: Dict[str, Any] = Field(default_factory=dict)


class LLMConversation(BaseModel):
    """
    Represents a complete conversation or workflow, acting as a container
    for an ordered sequence of LLMTurns.

    This is the top-level object retrieved from the storage system.
    """
    # The unique identifier for the entire conversation thread.
    conversation_id: str = Field(default_factory=lambda: str(uuid4()))

    # ID of the user associated with this conversation.
    user_id: Optional[str] = None
    
    # Generic ID for the primary entity (e.g., agent, workflow) this conversation belongs to.
    entity_id: Optional[str] = None

    # The unix timestamp (integer) when this conversation was created.
    created_at: int = Field(default_factory=lambda: int(time.time()))

    # The unix timestamp (integer) when this conversation was last updated
    # (e.g., when the last turn was appended).
    updated_at: int = Field(default_factory=lambda: int(time.time()))

    # The ordered list of interaction turns that make up this conversation.
    # In some storage implementations (e.g., SQL), this list is constructed
    # at read-time via a JOIN query.
    turns: List[LLMTurn] = Field(default_factory=list)

    # An optional, potentially auto-generated, summary of the conversation.
    summary: Optional[str] = None

    # A list of searchable string tags for categorization and filtering.
    tags: List[str] = Field(default_factory=list)
    
    # A flexible key-value store for conversation-level metadata.
    metadata: Dict[str, Any] = Field(default_factory=dict)